---
title: Hugging Face é¡¹ç›®æ¢³ç†
author: zhang
date: 2023-07-11 15:00:00 +0800
categories: [Blogging, Hugging Face]
tags: [huggingface]
---

## Hugging Face ä¸»è¦é¡¹ç›®å…¨æ™¯å›¾

Hugging Face æä¾›äº†ä¸€æ•´å¥— LLM å¼€å‘å·¥å…·é“¾ï¼Œä»æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¼€å‘ç®¡ç†ã€åˆ°åº”ç”¨æ„å»ºéƒ½æœ‰ç›¸å…³é¡¹ç›®è¿›è¡Œæ”¯æŒï¼Œè€Œä¸”è¿˜ä¸è‡ªå®¶å®˜ç½‘æä¾›çš„æ•°æ®/æ¨¡å‹ä»“åº“ã€åº”ç”¨éƒ¨ç½²æœåŠ¡æ— ç¼å¯¹æ¥ã€‚ä¸‹å›¾ç®€å•æ•´ç†äº† Hugging Face ä¸»è¦ä»“ä¹‹é—´çš„å…³ç³»ï¼Œé»„è‰²æ˜¯ Hugging Face è‡ªå®¶é¡¹ç›®ï¼Œè“è‰²æ˜¯å…¶å®ƒä¸‰æ–¹é¡¹ç›®

![ssh remote](/images/2023-07/huggingface_repos.png)

å¤§è‡´ä¾èµ–å…³ç³»ä»å·¦åˆ°å³ï¼Œä»ä¸‹åˆ°ä¸Šï¼Œ`Transformers` ä½œä¸ºæ ¸å¿ƒåº“æä¾›æ¨¡å‹ç»“æ„å®šä¹‰ä»¥åŠè¿è¡Œæ–¹æ³•ï¼Œ`Accelerate `æä¾›åˆ†å¸ƒå¼è®­ç»ƒèƒ½åŠ›ï¼Œ`PEFT` ç”¨æ¥åšæ¨¡å‹é«˜æ•ˆå¾®è°ƒï¼Œ`Optimum` ä¸»è¦é’ˆå¯¹ç‰¹å®šç¡¬ä»¶åšæ€§èƒ½ä¼˜åŒ–ï¼Œå·¦è¾¹ä¸€åˆ—ä¸»è¦æ˜¯æ•°æ®é›†ã€è¯„ä»·æŒ‡æ ‡ã€åˆ†è¯ç­‰è¾…åŠ©åº“ï¼Œä¸Šå±‚é€šè¿‡ `huggingface_hub` ä¸è¿œç«¯ä»“åº“è¿›è¡Œæ¨¡å‹/æ•°æ®çš„ä¸Šä¼ å’Œä¸‹è½½ï¼Œ`Text Generation Inference` å¯ä»¥å®ç°æ¨¡å‹çš„éƒ¨ç½²ï¼Œé€šè¿‡ RestFul API è¿›è¡Œè°ƒç”¨ï¼Œæœ€ä¸Šå±‚è¿˜æä¾›äº† `Chat UI` æ¥æ„å»º Web åº”ç”¨

é™¤äº†è‡ªå®¶é¡¹ç›®ï¼Œä¹Ÿæœ‰ä¸å°‘é«˜è´¨é‡çš„å¤–éƒ¨é¡¹ç›®é›†æˆäº† Hugging Face æˆ–è¢« Hugging Face ä¾èµ–ï¼Œä¾‹å¦‚æœ€åº•å±‚çš„ AI æ¡†æ¶ `PyTorch`, `TensorFLow`, `JAX`ï¼Œå†…å­˜ä¼˜åŒ–åº“ `DeepSpeed`ï¼ŒRuntime æ‰§è¡ŒåŠ é€Ÿåº“ `Onnxruntime`, `OpenVINO`ï¼Œåˆ†è¯å·¥å…· `NLTK`, `sentencepiece`ï¼Œå¼ºåŒ–å­¦ä¹  RLHF åº“ `TRL` å’Œ `TRLX`ï¼Œè¿˜æœ‰ä¸“é—¨é€šè¿‡ LoRA ç­‰æŠ€æœ¯åœ¨ä¸æ”¹å˜æ¨¡å‹æƒé‡çš„æ¡ä»¶ä¸‹ï¼Œåšè½»é‡åŒ–é€‚é…å±‚çš„ `Adapter Transformers` ç­‰




## ä¸»è¦é¡¹ç›®ä»‹ç»

### 1. Transformers

- æä¾›ä¸Šåƒç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ç”¨äºæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘é¢†åŸŸ
- æä¾›æ¨¡å‹ä¸‹è½½ä¸ä¸Šä¼ æ¥å£ï¼Œæä¾›è®­ç»ƒæ¥å£
- æ”¯æŒä¸‰ç§åç«¯ `PyTorch`, `TensorFlow` å’Œ `JAX`

æ¨¡å‹ä¸‹è½½ä¸ä½¿ç”¨æ ·ä¾‹

```python
from transformers import AutoTokenizer, GPT2Model
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = GPT2Model.from_pretrained("gpt2")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
```

æ¨¡å‹è®­ç»ƒæ ·ä¾‹

```python
from transformers import TrainingArguments, Trainer, logging

logging.set_verbosity_error()

training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)
```

æ¨¡å‹ä¸Šä¼ æ ·ä¾‹

```python
# ç”¨æ³•1: è®­ç»ƒåä¸Šä¼ 
training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.push_to_hub()

# ç”¨æ³•2: ç›´æ¥ä¸Šä¼ 
pt_model.push_to_hub("my-awesome-model")
```

### 2. Accelerate

- åˆ†å¸ƒå¼è®­ç»ƒåº“ï¼Œæ”¯æŒ `DeepSpeed` åç«¯è¿›è¡Œæ˜¾å­˜ä¼˜åŒ–
- åœ¨ä¸æ”¹å˜åŸæœ‰ `PyTorch` ä»£ç é€»è¾‘åŸºç¡€ä¸Šï¼Œä»…éœ€æ·»åŠ å‡ è¡Œä»£ç 
- æ”¯æŒå¤šç§ç¡¬ä»¶çš„å•æœº/å¤šæœºè®­ç»ƒ

è®­ç»ƒä»£ç ä¸ `PyTorch` å¯¹æ¯”æ ·ä¾‹

```diff
  import torch
  import torch.nn.functional as F
  from datasets import load_dataset
+ from accelerate import Accelerator

- device = 'cpu'
+ accelerator = Accelerator()

- model = torch.nn.Transformer().to(device)
+ model = torch.nn.Transformer()
  optimizer = torch.optim.Adam(model.parameters())

  dataset = load_dataset('my_dataset')
  data = torch.utils.data.DataLoader(dataset, shuffle=True)

+ model, optimizer, data = accelerator.prepare(model, optimizer, data)

  model.train()
  for epoch in range(10):
      for source, targets in data:
-         source = source.to(device)
-         targets = targets.to(device)

          optimizer.zero_grad()

          output = model(source)
          loss = F.cross_entropy(output, targets)

-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
```

### 3. PEFT

- é«˜æ•ˆå¤§æ¨¡å‹å¾®è°ƒåº“ï¼Œå¾®è°ƒå±€éƒ¨å‚æ•°ï¼Œä»¥é™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬
- æ”¯æŒ LoRA, Prefix Tuning,	P-Tuning ç­‰5ç§ä¸»æµå¾®è°ƒæ–¹æ³•

åˆ©ç”¨ `PEFT` æ„å»ºè®­ç»ƒå‚æ•°æ ·ä¾‹

```python
from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
model_name_or_path = "bigscience/mt0-large"
tokenizer_name_or_path = "bigscience/mt0-large"

peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282
```

é€šè¿‡è®­ç»ƒå‚æ•°å æ¯”å¯ä»¥çœ‹å‡ºï¼Œè¯¥åº“ä¸ºåœ¨æ¶ˆè´¹çº§ GPU å¡ä¸Šå¾®è°ƒæ¨¡å‹æä¾›å¯èƒ½

Hardware: Single A100 80GB GPU with CPU RAM above 64GB
|  Model   | Full Finetuning  | PEFT-LoRA PyTorch  | PEFT-LoRA DeepSpeed with CPU Offloading  |
|  ----  | ----  | ----  | ----  |
| bigscience/T0_3B (3B params)  | 47.14GB GPU / 2.96GB CPU | 14.4GB GPU / 2.96GB CPU | 9.8GB GPU / 17.8GB CPU |
| bigscience/mt0-xxl (12B params)  | OOM GPU | 56GB GPU / 3GB CPU | 22GB GPU / 52GB CPU |
| bigscience/bloomz-7b1 (7B params)  | OOM GPU | 32GB GPU / 3.8GB CPU | 18.1GB GPU / 35GB CPU |

### 4. Optimum

- `Transformers` å’Œ `Diffuser` è®­ç»ƒæ¨ç†ä¼˜åŒ–å·¥å…·ï¼Œå¯ä»¥æœ€å¤§é™åº¦åœ°æé«˜åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šè®­ç»ƒå’Œè¿è¡Œæ¨¡å‹çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ˜“ç”¨æ€§
- Optimum ä»“åº“åŸç”Ÿæ”¯æŒåç«¯å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼Œå…¶å®ƒç¡¬ä»¶è®¾å¤‡çš„é€‚é…å¯ä»¥å¦å¤–æ„å»º `Optimum-X` ä»“åº“ï¼Œä¾‹å¦‚ `optimum-habana` é’ˆå¯¹ HPU ç¡¬ä»¶è¿›è¡ŒåŠ é€Ÿ
  
åŸç”Ÿæ”¯æŒåç«¯

|  Accelerator   | Installation  |
|  ----  | ----  |
| ONNX Runtime | python -m pip install optimum[onnxruntime] |
| Intel Neural Compressor | python -m pip install optimum[neural-compressor] |
| IOpenVINO| python -m pip install optimum[openvino,nncf] |
| Habana Gaudi Processor (HPU) | python -m pip install optimum[habana] |

åç«¯æ”¯æŒç‰¹æ€§å¯¹æ¯”

| Features | ONNX Runtime | Neural Compressor | OpenVINO | TensorFlow Lite |
| ---- | ---- | ---- | ---- | ---- |
| Graph optimization |  âœ”ï¸ | N/A |  âœ”ï¸ | N/A |
| Post-training dynamic quantization | âœ”ï¸ | âœ”ï¸ | N/A | âœ”ï¸ |
| Post-training static quantization | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
| Quantization Aware Training (QAT) | N/A | âœ”ï¸ | âœ”ï¸ | N/A |
| FP16 (half precision) | âœ”ï¸ | N/A | âœ”ï¸ | âœ”ï¸ |
| Pruning | N/A | âœ”ï¸ | âœ”ï¸ | N/A |
| Knowledge Distillation | N/A | âœ”ï¸ | âœ”ï¸ | N/A |

OpenVINO åç«¯æ ·ä¾‹

```diff
- from transformers import AutoModelForSequenceClassification
+ from optimum.intel import OVModelForSequenceClassification
  from transformers import AutoTokenizer, pipeline

  model_id = "distilbert-base-uncased-finetuned-sst-2-english"
  tokenizer = AutoTokenizer.from_pretrained(model_id)
- model = AutoModelForSequenceClassification.from_pretrained(model_id)
+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)
  model.save_pretrained("./distilbert")

  classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
  results = classifier("He's a dreadful magician.")
```

ONNX Runtime åç«¯æ ·ä¾‹

```diff
- from transformers import AutoModelForQuestionAnswering
+ from optimum.onnxruntime import ORTModelForQuestionAnswering
  from transformers import AutoTokenizer, pipeline

  model_id = "deepset/roberta-base-squad2"
  tokenizer = AutoTokenizer.from_pretrained(model_id)
- model = AutoModelForQuestionAnswering.from_pretrained(model_id)
+ model = ORTModelForQuestionAnswering.from_pretrained("roberta_base_qa_onnx")
  qa_pipe = pipeline("question-answering", model=model, tokenizer=tokenizer)
  question = "What's Optimum?"
  context = "Optimum is an awesome library everyone should use!"
  results = qa_pipe(question=question, context=context)
```

### 5. Datasets

- å…¬å¼€æ•°æ®é›†ä¸‹è½½å’Œä¸Šä¼ ï¼Œæ„å»º data_loader æ•°æ®åŠ è½½å™¨
- é«˜æ•ˆçš„æ•°æ®é¢„å¤„ç†ï¼Œå¯¹ CSVã€JSONã€textã€PNGã€JPEGã€WAVã€MP3ã€Parquet ç­‰æœ¬åœ°æ•°æ®é›†è¿›è¡Œç®€å•ã€å¿«é€Ÿå’Œå¯å¤åˆ¶çš„æ•°æ®é¢„å¤„ç†å™¨
- åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†

æ•°æ®é›†ä¸‹è½½æ ·ä¾‹

```python
from datasets import load_dataset

# ä¸€æ¬¡æ€§ä¸‹è½½
dataset = load_dataset("rotten_tomatoes", split="train")

# æ¯æ¬¡å–ä¸€ä¸ªæ ·æœ¬
iterable_dataset = load_dataset("food101", split="train", streaming=True)
for example in iterable_dataset:
    print(example)

# å–å‰3ä¸ªæ ·æœ¬
list(iterable_dataset.take(3))
```

å›¾ç‰‡é¢„å¤„ç†æ ·ä¾‹

```python
def transforms(examples):
    examples["pixel_values"] = [image.convert("RGB").resize((100,100)) for image in examples["image"]]
    return examples

# ç”¨æ³•1
dataset = dataset.map(transforms, remove_columns=["image"], batched=True)

# æ–¹æ³•2
dataset.set_transform(transforms)
```

### 6. Tokenizers

- æ”¯æŒåˆ†è¯å™¨è®­ç»ƒ
- å®ç°å¿«é€Ÿåˆ†è¯

åŠ è½½ä¸è®­ç»ƒåˆ†è¯å™¨æ ·ä¾‹

```python
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("bert-base-uncased")
```

è®­ç»ƒåˆ†è¯å™¨æ ·ä¾‹

```python
from tokenizers.trainers import BpeTrainer

trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
tokenizer.train(files=["wiki.train.raw", "wiki.valid.raw", "wiki.test.raw"], trainer=trainer)
```

åˆ†è¯æ ·ä¾‹

```python
output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]
```
